#!/usr/bin/env python
"""Importation script that handles retrieving user-input for initializing data"""

import argparse
import os
import shutil
import datetime
import json
from dbconnect import connect

from pprint import pprint

def process(args, conn):
  """Method to preprocess dataset to parse out the necessary """
  # try:
  #   with open(args.meta, 'r') as mfp:
  #     m = json.load(mfp)

  #     pprint(m)
  # except:
  #   raise

  # NOTE: Example of connecting to the database to pull a bit of info
  # This can also be returned from the database as part of the sql statement
  # like the ones that Molly wrote
  # # Okay, SSH tunneled and it works just fine
  # # I mean, I could implement my own SSH tunneling like DBeaver
  # cur = conn.cursor()
  # sql = """SELECT * FROM chromosome;"""
  # cur.execute(sql)
  # rows = cur.fetchall()
  # for row in rows:
  #   pprint(row)
  # cur.close()


  # Dependency rundown
  # Independent values -- require extraction or user-input
  #   NAME                           ABBR     DEP(S)
  #   Location                       L
  #   Species                        S
  #   Traits                         T
  #   Growout_type                   GT
  #   GWAS_algorithm                 GA
  #   Imputation_method              IM
  #   Kinship_algorithm              KA
  #   Population_structure_algortihm PSA
  # 
  # One Independent Dependency
  #   Population                     P        S
  #   Chromosome                     C        S
  #   Kinship                        K        KA
  #   Population_structure           PS       PSA
  # 
  # One Dependent Dependency
  #   Line                           L        P
  # 
  # Two Dependencies (Mixed only)
  #   Genotype_version               GV       L, P
  #   Variant                        V        S, C
  #   Phenotype                      PH       L, T
  # 
  # Three Dependencies (Mixed only)
  #   Genotype                       G        L, C, GV
  #   Growout                        GO       L, P, GT
  # 
  # Many Dependencies (Mixed only)
  #   GWAS_run                       GRUN     T, GA, GV, IM, K, PS
  #   GWAS_result                    GRES     C, GRUN


  # ============= Independent Values =============
  # Get all of the independent values
  # NOTE: this includes finding them if the value already exists in the database
  # Location
  #   req. user input: country
  #   These values can be extracted from phenotype files
  #   Their filenames include the location code
  # NOTE(timp): 'Combined' location exists, but unclear what it represents
  # 

  # Species
  #   req. user input: shortname, binomial
  # Traits
  #   req. user input: none
  # Growout_type
  #   req. user input: name
  # GWAS_Algorithm
  #   req. user input: name
  # Imputation_method
  #   req. user input: name
  # Kinship_algorithm
  #   req. user input: name
  # Population_structure_algorithm
  #   req. user input: name

  # ============= Single 'Independency' =============
  # Population
  #   req. user input: name
  # Chromosome
  #   req. user input: ??? (uncertain if there is a naming convention)
  # Kinship
  #   req. user input: none (filepath generated by this script/target host)
  # Population_structure
  #   req. user input: none (filepath generated by this script/target host)
  
  # ============= Single Dependency =============
  # Line
  #   req. user input: none (lines can be pulled from 012 files)

  # ============= Two Dependencies =============
  # Genotype_version
  #   req. user input: ??? (does this require a lookup of the line?)
  # Variant
  #   req. user input: none (variants can be pulled from 012.pos file)
  # Phenotype
  #   req. user input: none (phenotype measurement value can be pulled from parsed pheno files)
  #   The parsed pheno files originated from CSV transform on 5.mergedWeightNorm...longFormat.csv

  # ============= Three Dependencies =============
  # Genotype
  #   req. user input: none (genotypes can be pulled from .012 and .012.indv files)
  #   This will happen for each chromosome (and possibly scaffold) specified prior
  # Growout
  # NOTE(timp): Consider adding a new constraint to the growout to guarantee the uniqueness
  #             of (population, location, year, and growout_type)
  #             Should this mean that the name should *not* be unique?
  #   

  # ============= Many Dependencies =============
  # GWAS_run
  #   req. user input: missing SNP cutoff value, missing line cutoff value, minor allele cutoff value
  # GWAS_result
  # NOTE(timp): For some reason the minor allele cutoff value was placed in a different spot than that in the GWAS_run table
  #   req. user input: missing SNP cutoff value, missing line cutoff value, minor allele cutoff value







  pass


def parseOptions():
  """
  Function to parse user-provided options from terminal
  """
  parser = argparse.ArgumentParser()
  # parser.add_argument("-m", "--meta", required = True,
  #                     help="JSON format file that contains metadata about the dataset to import.")
  parser.add_argument("-v", "--verbose", action="store_true",
                      help="Increase output verbosity")
  parser.add_argument("-o", "--outdir", default = f"output_{datetime.datetime.now().strftime('%Y_%m_%d_%H_%M_%S')}",
                      help="Path of output directory")
  parser.add_argument("--debug", action = "store_true", help = "Enables --verbose and disables writes to disk")
  args = parser.parse_args()
  if args.debug is True:
    args.verbose = True
    args.write = False
  
  return args

if __name__ == "__main__":
  args = parseOptions()
  try:
    conn = connect()
    process(args, conn)
  except:
    raise
